{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (Activation, Conv2D, Conv3D, Dense, Dropout, Flatten,\n",
    "                          Input, Lambda, MaxPooling2D)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from skimage.restoration import (denoise_bilateral, denoise_tv_chambolle,\n",
    "                                 denoise_wavelet, estimate_sigma)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from vis.regularizers import LPNorm, TotalVariation\n",
    "from vis.utils import utils\n",
    "from vis.visualization import (visualize_activation,\n",
    "                               visualize_activation_with_losses)\n",
    "from tensorflow.contrib import graph_editor as ge\n",
    "from cleverhans.attacks import (FastGradientMethod, MadryEtAl,\n",
    "                                ProjectedGradientDescent, SparseL1Descent)\n",
    "from cleverhans.model import Model as CHModel\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from attribution.InfluenceInvariants import InfluenceInvariants\n",
    "from attribution.ActivationInvariants import ActivationInvariants\n",
    "from attribution.invariant_utils import (probits_from_invariants,\n",
    "                                         smooth_logits_from_invariants,\n",
    "                                         smooth_logit_tensor_from_invariants,\n",
    "                                         smooth_logit_tensor_from_invariant,\n",
    "                                         smooth_probits_from_invariants,\n",
    "                                         smooth_probit_tensor_from_invariants,\n",
    "                                         tally_total_stats)\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('tensorflow').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('cleverhans').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\n",
    "\n",
    "# Use only classes that have at least 100 images\n",
    "# There are five such classes in LFW\n",
    "lfw_slice = (slice(68, 196, None), slice(61, 190, None))\n",
    "faces_data = fetch_lfw_people(min_faces_per_person=100, color=True, slice_=lfw_slice)\n",
    "images = faces_data.images\n",
    "n_classes = faces_data.target.max()+1\n",
    "x, y = faces_data.data, keras.utils.to_categorical(faces_data.target, n_classes)\n",
    "images /= 255.0\n",
    "\n",
    "# Use 3/4 for training, the rest for testing\n",
    "N_tr = int(len(x)*0.75)\n",
    "N_te = len(x) - N_tr\n",
    "x_tr, y_tr = x[:N_tr], y[:N_tr]\n",
    "x_te, y_te = x[N_tr:], y[N_tr:]\n",
    "im_tr, im_te = images[:N_tr], images[N_tr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "logits (Dense)               (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 101,589\n",
      "Trainable params: 101,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy:\n",
      "train=1.0\n",
      "test=0.81\n"
     ]
    }
   ],
   "source": [
    "inp = keras.layers.Input(shape=im_tr[0].shape, name='features')\n",
    "out = keras.layers.Conv2D(128, (3,3), activation='relu')(inp)\n",
    "out = keras.layers.MaxPooling2D(pool_size=(2,2))(out)\n",
    "out = keras.layers.Conv2D(64, (3,3), activation='relu')(out)\n",
    "out = keras.layers.MaxPooling2D(pool_size=(2,2))(out)\n",
    "out = keras.layers.Conv2D(32, (3,3), activation='relu')(out)\n",
    "out = keras.layers.MaxPooling2D(pool_size=(2,2))(out)\n",
    "out = keras.layers.Conv2D(16, (3,3), activation='relu')(out)\n",
    "out = keras.layers.MaxPooling2D(pool_size=(2,2))(out)\n",
    "out = keras.layers.Flatten()(out)\n",
    "out = keras.layers.Dense(16, activation='relu')(out)\n",
    "out = keras.layers.Dense(y[0].shape[0], name='logits')(out)\n",
    "out = keras.layers.Activation('softmax', name='softmax')(out)\n",
    "model = keras.Model(inp, out)\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "model.load_weights('weights/lfw-small-tf.h5')\n",
    "print('accuracy:')\n",
    "print('train={:.2}'.format(model.evaluate(im_tr, y_tr, verbose=False)[1]))\n",
    "print('test={:.2}'.format(model.evaluate(im_te, y_te, verbose=False)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below, `get_random_invariants`, does the following:\n",
    "1. Samples $n$ uniform-random points from [0,1] into $x$\n",
    "2. For each class label $l$, runs projected gradient descent (`MadryEtAl` from `cleverhans`) with both $L_\\infty$ and $L_2$ norm over $x$ to create approximately $2n$ points with label $l$, stored in $x_l$\n",
    "3. Concatenates all the labeled random points $x_1, \\ldots, x_l$ into $x'$\n",
    "4. For each layer $t$ specified in `layers`, generate a set of invariants using $x'$\n",
    "\n",
    "Importantly, for influence invariants we exclude activation information (i.e., `multiply_attributions=False` in the invariant constructor). The reason for this will become clear later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel(CHModel):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        del kwargs\n",
    "        CHModel.__init__(self, 'model_b', model.output_shape[1], locals())\n",
    "\n",
    "        self.model = model\n",
    "        self.fprop(tf.placeholder(tf.float32, (128,)+model.input_shape[1:]))\n",
    "\n",
    "    def fprop(self, x, **kwargs):\n",
    "        del kwargs\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = K.variable(x)\n",
    "        with tf.variable_scope(self.scope, reuse=tf.AUTO_REUSE):\n",
    "            return dict([(layer.name, keras.Model(self.model.input, layer.output)(x)) for layer in self.model.layers])\n",
    "\n",
    "def get_random_invariants(model, layers, n, x_shape, n_classes, nbiter=50, eps_linf=0.1, eps_l2=5., agg_fn=None, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    rand_data = []\n",
    "    cls_size = n\n",
    "    rand_x = np.random.uniform(size=(cls_size,) + x_shape)\n",
    "    nbiter = 50\n",
    "    for l in range(n_classes):\n",
    "        model_ch = KerasModel(model)\n",
    "        sess = K.get_session()\n",
    "        pgd_params_l2 = {'eps': eps_l2,\n",
    "                         'y_target': to_categorical(np.zeros((1,)) + l, n_classes),\n",
    "                         'eps_iter': eps_l2 / nbiter,\n",
    "                         'nb_iter': nbiter,\n",
    "                         'ord': 2,\n",
    "                         'clip_min': 0.,\n",
    "                         'clip_max': 1.}\n",
    "        pgd_params_linf = {'eps': eps_linf,\n",
    "                           'y_target': to_categorical(np.zeros((1,)) + l, n_classes),\n",
    "                           'eps_iter': eps_linf / nbiter,\n",
    "                           'nb_iter': nbiter,\n",
    "                           'ord': np.inf,\n",
    "                           'clip_min': 0.,\n",
    "                           'clip_max': 1.}\n",
    "        pgd = MadryEtAl(model_ch, sess=sess)\n",
    "        cur_data = []\n",
    "        cur_data.append(pgd.generate_np(rand_x, **pgd_params_l2))\n",
    "        cur_data.append(pgd.generate_np(rand_x, **pgd_params_linf))\n",
    "        rand_data.append(np.concatenate(cur_data, axis=0))\n",
    "        print('generated class', l)\n",
    "\n",
    "    rand_data = np.concatenate(rand_data)\n",
    "\n",
    "    log_model = Model(model.inputs, model.layers[-2].output)\n",
    "    log_model.layers[-1].activation = keras.activations.softplus\n",
    "    log_model = utils.apply_modifications(log_model)\n",
    "\n",
    "    gens = [InfluenceInvariants(log_model, layer=target, agg_fn=agg_fn,\n",
    "                                multiply_activation=False).compile() for target in layers]\n",
    "    invs_by_layer = [gen.get_invariants(rand_data, batch_size = 1) for gen in gens]\n",
    "\n",
    "    return invs_by_layer, gens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence invariants\n",
    "\n",
    "We'll start with `agg_fn=None`, so the invariants refer to specific locations on the feature maps. We'll test all convolutional layers (1-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated class 0\n",
      "generated class 1\n",
      "generated class 2\n",
      "generated class 3\n",
      "generated class 4\n"
     ]
    }
   ],
   "source": [
    "invs_by_layer, gens = get_random_invariants(model, \n",
    "                                            layers, \n",
    "                                            256, \n",
    "                                            im_tr[0].shape, \n",
    "                                            5, \n",
    "                                            agg_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invs_by_layer[0]), len(invs_by_layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(conv2d_4[133] > 0.0 &\n",
      " conv2d_4[43] > 0.0 &\n",
      " conv2d_4[1] > 0.0)\n",
      "\t--> Q = 4\n",
      "support=0.0148, precision=1.0\n",
      "(max_pooling2d_4[42] > 0.0 &\n",
      " max_pooling2d_4[43] > 0.0 &\n",
      " max_pooling2d_4[40] > 0.0 &\n",
      " max_pooling2d_4[18] > 0.0)\n",
      "\t--> Q = 0\n",
      "support=0.00193, precision=1.0\n"
     ]
    }
   ],
   "source": [
    "print(invs_by_layer[0][0])\n",
    "print(invs_by_layer[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training set results, layer 7\n",
      "# invariants per class: {0: 4, 1: 5, 2: 4, 3: 1, 4: 2}\n",
      "support by class: {0: 0.9836956521739131, 1: 0.9148936170212766, 2: 0.9974811083123426, 3: 0.8888888888888888, 4: 0.8585858585858586}\n",
      "precision by class: {0: 0.8916256157635468, 1: 1.0, 2: 0.9974811083123426, 3: 0.972972972972973, 4: 0.8947368421052632}\n",
      "overall prediction accuracy: 0.959\n",
      "---------- test set results, layer 7\n",
      "# invariants per class: {0: 4, 1: 5, 2: 4, 3: 1, 4: 2}\n",
      "support by class: {0: 1.0, 1: 0.9142857142857143, 2: 1.0, 3: 0.9705882352941176, 4: 0.8787878787878788}\n",
      "precision by class: {0: 0.8727272727272727, 1: 1.0, 2: 1.0, 3: 1.0, 4: 0.9666666666666667}\n",
      "overall prediction accuracy: 0.804\n",
      "---------- training set results, layer 8\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 2, 4: 1}\n",
      "support by class: {0: 1.0, 1: 0.9893617021276596, 2: 0.9924433249370277, 3: 0.7530864197530864, 4: 1.0}\n",
      "precision by class: {0: 0.9945945945945946, 1: 0.9893617021276596, 2: 1.0, 3: 1.0, 4: 0.8319327731092437}\n",
      "overall prediction accuracy: 0.972\n",
      "---------- test set results, layer 8\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 2, 4: 1}\n",
      "support by class: {0: 1.0, 1: 1.0, 2: 0.9925925925925926, 3: 0.7352941176470589, 4: 1.0}\n",
      "precision by class: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 0.7857142857142857}\n",
      "overall prediction accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_te = im_tr, im_te\n",
    "inv_models = [probits_from_invariants(invs) for invs in invs_by_layer]\n",
    "for l in range(len(layers)):\n",
    "    invs = invs_by_layer[l]\n",
    "    inv_model = inv_models[l]\n",
    "    print('-' * 10, 'training set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_tr, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_tr).argmax(axis=1) == y_tr.argmax(axis=1)).mean()))\n",
    "\n",
    "    print('-' * 10, 'test set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_te, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_te).argmax(axis=1) == y_te.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: random influents invariants at layers 7, 8 achieve 95-97% training accuracy, and 80% test accuracy. The original model achieved 100% training accuracy and 81% test accuracy.\n",
    "\n",
    "Now we'll generate influence invariants with `agg_fn=K.sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated class 0\n",
      "generated class 1\n",
      "generated class 2\n",
      "generated class 3\n",
      "generated class 4\n"
     ]
    }
   ],
   "source": [
    "invs_by_layer, gens = get_random_invariants(model, \n",
    "                                            layers, \n",
    "                                            256, \n",
    "                                            im_tr[0].shape, \n",
    "                                            5, \n",
    "                                            agg_fn=K.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training set results, layer 7\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 6, 4: 3}\n",
      "support by class: {0: 0.717391304347826, 1: 0.7021276595744681, 2: 0.7531486146095718, 3: 0.9382716049382716, 4: 0.9393939393939394}\n",
      "precision by class: {0: 0.9850746268656716, 1: 0.9705882352941176, 2: 0.9835526315789473, 3: 0.39790575916230364, 4: 1.0}\n",
      "overall prediction accuracy: 0.836\n",
      "---------- test set results, layer 7\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 6, 4: 3}\n",
      "support by class: {0: 0.7291666666666666, 1: 0.7142857142857143, 2: 0.6814814814814815, 3: 1.0, 4: 1.0}\n",
      "precision by class: {0: 1.0, 1: 1.0, 2: 0.989247311827957, 3: 0.4, 4: 1.0}\n",
      "overall prediction accuracy: 0.667\n",
      "---------- training set results, layer 8\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 6, 4: 3}\n",
      "support by class: {0: 0.717391304347826, 1: 0.7021276595744681, 2: 0.7531486146095718, 3: 0.9382716049382716, 4: 0.7878787878787878}\n",
      "precision by class: {0: 0.9850746268656716, 1: 0.9705882352941176, 2: 0.9835526315789473, 3: 0.39790575916230364, 4: 1.0}\n",
      "overall prediction accuracy: 0.819\n",
      "---------- test set results, layer 8\n",
      "# invariants per class: {0: 3, 1: 3, 2: 4, 3: 6, 4: 3}\n",
      "support by class: {0: 0.7291666666666666, 1: 0.7142857142857143, 2: 0.6814814814814815, 3: 1.0, 4: 0.7575757575757576}\n",
      "precision by class: {0: 1.0, 1: 1.0, 2: 0.989247311827957, 3: 0.4, 4: 1.0}\n",
      "overall prediction accuracy: 0.649\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_te = im_tr, im_te\n",
    "inv_models = [probits_from_invariants(invs) for invs in invs_by_layer]\n",
    "for l in range(len(layers)):\n",
    "    invs = invs_by_layer[l]\n",
    "    inv_model = inv_models[l]\n",
    "    print('-' * 10, 'training set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_tr, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_tr).argmax(axis=1) == y_tr.argmax(axis=1)).mean()))\n",
    "\n",
    "    print('-' * 10, 'test set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_te, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_te).argmax(axis=1) == y_te.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that aggregating across spatial dimensions does not improve things. This is somewhat surprising especially in the lower recall results, as one might expect spatial dependencies to narrow the scope of an invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate activation invariants from random data instead of influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_invariants(model, layers, n, x_shape, n_classes, nbiter=50, eps_linf=0.1, eps_l2=5., agg_fn=None, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    rand_data = []\n",
    "    cls_size = n\n",
    "    rand_x = np.random.uniform(size=(cls_size,) + x_shape)\n",
    "    nbiter = 50\n",
    "    for l in range(n_classes):\n",
    "        model_ch = KerasModel(model)\n",
    "        sess = K.get_session()\n",
    "        pgd_params_l2 = {'eps': eps_l2,\n",
    "                         'y_target': to_categorical(np.zeros((1,)) + l, n_classes),\n",
    "                         'eps_iter': eps_l2 / nbiter,\n",
    "                         'nb_iter': nbiter,\n",
    "                         'ord': 2,\n",
    "                         'clip_min': 0.,\n",
    "                         'clip_max': 1.}\n",
    "        pgd_params_linf = {'eps': eps_linf,\n",
    "                           'y_target': to_categorical(np.zeros((1,)) + l, n_classes),\n",
    "                           'eps_iter': eps_linf / nbiter,\n",
    "                           'nb_iter': nbiter,\n",
    "                           'ord': np.inf,\n",
    "                           'clip_min': 0.,\n",
    "                           'clip_max': 1.}\n",
    "        pgd = MadryEtAl(model_ch, sess=sess)\n",
    "        cur_data = []\n",
    "        cur_data.append(pgd.generate_np(rand_x, **pgd_params_l2))\n",
    "        cur_data.append(pgd.generate_np(rand_x, **pgd_params_linf))\n",
    "        rand_data.append(np.concatenate(cur_data, axis=0))\n",
    "        print('generated class', l)\n",
    "\n",
    "    rand_data = np.concatenate(rand_data)\n",
    "\n",
    "    log_model = Model(model.inputs, model.layers[-2].output)\n",
    "    log_model.layers[-1].activation = keras.activations.softplus\n",
    "    log_model = utils.apply_modifications(log_model)\n",
    "\n",
    "    gens = [ActivationInvariants(log_model, layers=[target], agg_fn=agg_fn).compile() for target in layers]\n",
    "    invs_by_layer = [gen.get_invariants(rand_data, batch_size = 1) for gen in gens]\n",
    "\n",
    "    return invs_by_layer, gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated class 0\n",
      "generated class 1\n",
      "generated class 2\n",
      "generated class 3\n",
      "generated class 4\n"
     ]
    }
   ],
   "source": [
    "invs_by_layer, gens = get_random_invariants(model, \n",
    "                                            layers, \n",
    "                                            256, \n",
    "                                            im_tr[0].shape, \n",
    "                                            5, \n",
    "                                            agg_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training set results, layer 7\n",
      "# invariants per class: {0: 96, 1: 98, 2: 75, 3: 57, 4: 99}\n",
      "support by class: {0: 0.09239130434782608, 1: 0.20212765957446807, 2: 0.5113350125944585, 3: 0.14814814814814814, 4: 0.42424242424242425}\n",
      "precision by class: {0: 0.3148148148148148, 1: 0.2087912087912088, 2: 0.5152284263959391, 3: 0.13636363636363635, 4: 0.18421052631578946}\n",
      "overall prediction accuracy: 0.343\n",
      "---------- test set results, layer 7\n",
      "# invariants per class: {0: 96, 1: 98, 2: 75, 3: 57, 4: 99}\n",
      "support by class: {0: 0.0625, 1: 0.11428571428571428, 2: 0.4, 3: 0.058823529411764705, 4: 0.3939393939393939}\n",
      "precision by class: {0: 0.17647058823529413, 1: 0.12903225806451613, 2: 0.42857142857142855, 3: 0.06451612903225806, 4: 0.1625}\n",
      "overall prediction accuracy: 0.281\n",
      "---------- training set results, layer 8\n",
      "# invariants per class: {0: 10, 1: 0, 2: 10, 3: 16, 4: 5}\n",
      "support by class: {0: 0.09239130434782608, 1: 0, 2: 0.04785894206549118, 3: 0.012345679012345678, 4: 0.020202020202020204}\n",
      "precision by class: {0: 0.3953488372093023, 1: 0, 2: 0.5135135135135135, 3: 0.1111111111111111, 4: 0.08333333333333333}\n",
      "overall prediction accuracy: 0.198\n",
      "---------- test set results, layer 8\n",
      "# invariants per class: {0: 10, 1: 0, 2: 10, 3: 16, 4: 5}\n",
      "support by class: {0: 0.08333333333333333, 1: 0, 2: 0.022222222222222223, 3: 0.0, 4: 0.030303030303030304}\n",
      "precision by class: {0: 0.4444444444444444, 1: 0, 2: 0.2727272727272727, 3: 1.0, 4: 0.14285714285714285}\n",
      "overall prediction accuracy: 0.158\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_te = im_tr, im_te\n",
    "inv_models = [probits_from_invariants(invs) for invs in invs_by_layer]\n",
    "for l in range(len(layers)):\n",
    "    invs = invs_by_layer[l]\n",
    "    inv_model = inv_models[l]\n",
    "    print('-' * 10, 'training set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_tr, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_tr).argmax(axis=1) == y_tr.argmax(axis=1)).mean()))\n",
    "\n",
    "    print('-' * 10, 'test set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_te, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_te).argmax(axis=1) == y_te.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these results indicate, the property does not hold for activation invariants.\n",
    "\n",
    "As one final experiment, we'll try aggregating across spatial dimensions with activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated class 0\n",
      "generated class 1\n",
      "generated class 2\n",
      "generated class 3\n",
      "generated class 4\n"
     ]
    }
   ],
   "source": [
    "invs_by_layer, gens = get_random_invariants(model, \n",
    "                                            layers, \n",
    "                                            256, \n",
    "                                            im_tr[0].shape, \n",
    "                                            5, \n",
    "                                            agg_fn=K.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te = im_tr, im_te\n",
    "inv_models = [probits_from_invariants(invs) for invs in invs_by_layer]\n",
    "for l in range(len(layers)):\n",
    "    invs = invs_by_layer[l]\n",
    "    inv_model = inv_models[l]\n",
    "    print('-' * 10, 'training set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_tr, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_tr).argmax(axis=1) == y_tr.argmax(axis=1)).mean()))\n",
    "\n",
    "    print('-' * 10, 'test set results, layer', layers[l])\n",
    "    n_per, sup, prec = tally_total_stats(\n",
    "        invs, model, x_te, batch_size=1)\n",
    "    print('# invariants per class:', n_per)\n",
    "    print('support by class:', sup)\n",
    "    print('precision by class:', prec)\n",
    "    print('overall prediction accuracy: {:.3}'.format(\n",
    "        (inv_model(x_te).argmax(axis=1) == y_te.argmax(axis=1)).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
